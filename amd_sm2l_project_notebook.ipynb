{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "amd-sm2l-project-notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2JnyRXvNB8ID"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidegavio/amd-sm2l-project/blob/main/amd_sm2l_project_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoYTal8F893L"
      },
      "source": [
        "# Joint project AMD - SM2L\n",
        "Davide Gavio - 930569\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KprEsQWz7NTh"
      },
      "source": [
        "# Google Colab settings\n",
        "Those actions need to be done in order to have the notebook working on Google Colab. \n",
        "If the notebook is executed elsewhere skip to the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk1cDVP62_Rf"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz   \n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark pyspark seaborn kaggle scikit-learn matplotlib pandas progressbar2\n",
        "!rm spark-3.0.1-bin-hadoop2.7.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHnXTcoq7J5i"
      },
      "source": [
        "# PySpark settings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUn7Dsf99bRS"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('amd-sm2l-project').master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8inu3DYIcba"
      },
      "source": [
        "# Imports and settings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8ApGnI3If3d",
        "outputId": "c1ff84d8-f681-47c6-e40e-384b8d77f35d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"davidegavio\",\"key\":\"f4540434f20370f2bf34e2f9010b647e\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzUMYDbp2nx_"
      },
      "source": [
        "from matplotlib.pyplot import xlabel, ylabel\n",
        "import pandas as pd\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import DoubleType, Row\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Imputer, MinMaxScaler, OneHotEncoder, VectorAssembler, StringIndexer, StandardScaler\n",
        "from pyspark.mllib.linalg import DenseVector\n",
        "from pyspark.mllib.regression import LabeledPoint, RidgeRegressionWithSGD\n",
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.feature import ChiSqSelector, VectorIndexer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pyspark.sql.types import *\n",
        "import math\n",
        "import kaggle"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vNXHUuZ83BU"
      },
      "source": [
        "learning_rate = 0.000000001\n",
        "n_iterations = 1\n",
        "folds = 10\n",
        "lowest_error = float('inf')\n",
        "alphas = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]\n",
        "num_partitions = spark.sparkContext.defaultParallelism * 3"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo2O7olfI3zd",
        "outputId": "0f919cbd-c809-492d-c756-55c9d0983a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print('Parallelism info')\n",
        "print(\"Default parallelism: {}\".format(spark.sparkContext.defaultParallelism))\n",
        "print(\"Custom parallelism: {}\\n\".format(num_partitions))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parallelism info\n",
            "Default parallelism: 2\n",
            "Custom parallelism: 6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCoQaLls9pbz"
      },
      "source": [
        "# Ridge Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7K7Hh3i2nyT"
      },
      "source": [
        "import numpy as np \n",
        "import math\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from progressbar import ProgressBar\n",
        " \n",
        " \n",
        " \n",
        "class SparkRidgeRegression(object):\n",
        "    \"\"\" Base regression model. Models the relationship between a scalar dependent variable y and the independent \n",
        "        variables X. \n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_iterations: float\n",
        "            The number of training iterations the algorithm will tune the weights for.\n",
        "        learning_rate: float\n",
        "            The step length that will be used when updating the weights.\"\"\"\n",
        "    def __init__(self, n_iterations, learning_rate, reg_factor):\n",
        "        self.n_iterations = n_iterations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.reg_factor = reg_factor\n",
        "    \n",
        "    def get_training_errors(self):\n",
        "        return self.training_errors\n",
        "    \n",
        "    def set_training_errors(self, error):\n",
        "        self.training_errors = error\n",
        " \n",
        "    def squared_error(self, target, prediction):\n",
        "        return (target - prediction) ** 2\n",
        " \n",
        "    def root_mean_squared_error(self, predictions):\n",
        "        return np.sqrt(predictions.map(lambda p: self.squared_error(*p)).mean())\n",
        " \n",
        "    def mean_squared_error(self, predictions):\n",
        "        return predictions.map(lambda p: self.squared_error(p[0], p[1])).mean()\n",
        " \n",
        "    def mean_absolute_error(self, predictions):\n",
        "        return np.abs(predictions.map(lambda prediction: prediction[1] - prediction[0]).reduce(lambda a, b: a + b))/predictions.count()\n",
        "    \n",
        "    def r2(self, predictions):\n",
        "        mean_ = predictions.rdd.map(lambda t: t[0]).mean()\n",
        "        sum_squares = predictions.rdd.map(lambda t: (t[0] - mean_)**2).sum()\n",
        "        residual_sum_squares = predictions.rdd.map(lambda t: self.squared_error(*t)).sum()\n",
        "        return 1 - (residual_sum_squares / sum_squares)\n",
        " \n",
        " \n",
        "    def get_grad_sum(self, example):\n",
        "        return (self.weights.dot(DenseVector(example.features)) - example.label) * example.features\n",
        " \n",
        "    \n",
        "    def fit(self, observations):\n",
        "        progressbar = ProgressBar()\n",
        "        features_number = len(observations.take(1)[0].features)\n",
        "        self.training_errors = []\n",
        "        self.weights = np.zeros(features_number)        \n",
        "        start = datetime.now()\n",
        "        # Do gradient descent for n_iterations\n",
        "        for i in progressbar(range(self.n_iterations)):\n",
        "          # Get the prediction given an example and the current weights\n",
        "          predictions = observations.map(lambda example: self.predict(example)) # Result [label, prediction]\n",
        "          # Calculate l2 loss\n",
        "          regularization = self.reg_factor * self.weights\n",
        "          self.training_errors.append(self.root_mean_squared_error(predictions))\n",
        "          # Gradient of l2 loss w.r.t w\n",
        "          grad_w = observations.map(lambda example: DenseVector(self.get_grad_sum(example))).reduce(lambda x, y: x + y) + regularization\n",
        "          # Update the weights\n",
        "          self.weights -= self.learning_rate * grad_w\n",
        "          if i == self.n_iterations-1:\n",
        "            float_predictions = predictions.map(lambda xs: [float(x) for x in xs])\n",
        "            predictions_df = float_predictions.toDF(['label', 'predictions'])\n",
        "            print('From scratch training MSE: {}'.format(self.mean_squared_error(predictions=predictions)))\n",
        "            print('From scratch training RMSE: {}'.format(self.root_mean_squared_error(predictions=predictions)))\n",
        "            print('From scratch training MAE: {}'.format(self.mean_absolute_error(predictions=predictions)))\n",
        "            print('From scratch training R2: {}'.format(self.r2(predictions_df)))\n",
        " \n",
        "    def predict(self, example):\n",
        "        return (example.label, self.weights.dot(DenseVector(example.features)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHV_XrIA9xcy"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8LrT0C62nyg"
      },
      "source": [
        "def to_labeledpoint(row):\n",
        "  row_dict = row.asDict()\n",
        "  target = np.array(row_dict['target'][0])\n",
        "  features = np.array(row_dict['scaled_features']).tolist()\n",
        "  features.insert(0, 1.0)\n",
        "  return LabeledPoint(target, features)\n",
        " \n",
        "def remove_outliers(df):\n",
        "  quantiles = df.approxQuantile(label_to_predict, [0.25, 0.75], 0.1)\n",
        "  iqr = quantiles[1] - quantiles[0]\n",
        "  if iqr != 0:\n",
        "    df = df.filter(f.col(label_to_predict).between(quantiles[0] - (1.5 * iqr), quantiles[1] + (1.5 * iqr)))\n",
        "  return df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewvj9CPvCbEQ"
      },
      "source": [
        "# Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcFw1LhWCj0n",
        "outputId": "aa5e5a9d-fa6b-4067-dad1-129772b736e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print('Downloading necessary files from Kaggle')\n",
        "startTime = datetime.now()\n",
        "kaggle.api.authenticate()\n",
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"davidegavio\",\"key\":\"f4540434f20370f2bf34e2f9010b647e\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!mkdir ./datasets\n",
        "!kaggle datasets download census/2013-american-community-survey -p ./datasets\n",
        "with ZipFile('./datasets/2013-american-community-survey.zip', 'r') as zipObj:\n",
        "   zipObj.extractall('./datasets/2013-american-community-survey')\n",
        "os.remove('./datasets/2013-american-community-survey.zip')\n",
        "print('Your download has been completed in: {}'.format(datetime.now() - startTime))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading necessary files from Kaggle\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "mkdir: cannot create directory ‘./datasets’: File exists\n",
            "Downloading 2013-american-community-survey.zip to ./datasets\n",
            " 99% 909M/916M [00:05<00:00, 179MB/s]\n",
            "100% 916M/916M [00:05<00:00, 170MB/s]\n",
            "Your download has been completed in: 0:01:06.394393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj4yxo3290hH"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By7ssonw2nys",
        "outputId": "33c02ecf-fb19-4d67-af4c-03f2c072671f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%%time\n",
        "print('Reading from csv')\n",
        "df_a = spark.read.csv('/content/datasets/2013-american-community-survey/ss13pusa.csv', inferSchema=True, header=True).limit(35000)\n",
        "# df_a = df_a.sample(False, 0.075, 6)\n",
        "df_b = spark.read.csv('/content/datasets/2013-american-community-survey/ss13pusb.csv', inferSchema=True, header=True).limit(35000)\n",
        "# df_b = df_b.sample(False, 0.075, 6)\n",
        "df = df_a.union(df_b)\n",
        "df = df.repartition(numPartitions=num_partitions)\n",
        "label_to_predict = 'WAGP'\n",
        "drop_thresh = .66\n",
        "print(f\"The shape is {df.count():d} rows by {len(df.columns):d} columns.\")\n",
        "print('The dataframe is divided in {} partitions'.format(df.rdd.getNumPartitions()))\n",
        "print('==> Done\\n')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading from csv\n",
            "The shape is 70000 rows by 283 columns.\n",
            "The dataframe is divided in 6 partitions\n",
            "==> Done\n",
            "\n",
            "CPU times: user 141 ms, sys: 24.4 ms, total: 165 ms\n",
            "Wall time: 1min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlFlUp4vtBlb",
        "outputId": "829b49e2-af06-4524-fdcc-c24204051101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "df.select(label_to_predict).describe().show()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|summary|              WAGP|\n",
            "+-------+------------------+\n",
            "|  count|             58107|\n",
            "|   mean|20750.508716677854|\n",
            "| stddev| 37349.73921337099|\n",
            "|    min|                 0|\n",
            "|    max|            327000|\n",
            "+-------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XwZz1ei2ny4",
        "outputId": "d2a703ea-20a7-469c-fe1e-a34c5741a612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%%time\n",
        "print('Dropping useless columns, columns and rows with more than {}% of null values'.format(drop_thresh*100))\n",
        "df = df.select(*(f.col(c).cast(\"float\").alias(c) for c in df.columns)) # Casting the dataframe to float\n",
        "df = df.drop('RT', 'PINCP ', 'PERNP' ) # Removing unnecessary columns\n",
        "count_before = df.count()\n",
        "thresh = int(len(df.columns)*drop_thresh)\n",
        "df = df.dropna(thresh=thresh) # Dropping rows with more than 66% of null values\n",
        "print('Dropped {} rows with less than {}% of non-null values'.format(count_before-df.count(), drop_thresh*100))\n",
        "print('Keeping only rows with non-null value in target position')\n",
        "count_before = df.count()\n",
        "df = df.filter(df[label_to_predict].isNotNull()) # Keeping only the rows with not-null values in the corresponding label\n",
        "data_agg = df.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in df.columns])\n",
        "print('Dropped {} rows with with null in {} column'.format(count_before-df.count(), label_to_predict))\n",
        "count_before = df.count()\n",
        "col_before = len(df.columns)\n",
        "over_thresh_col = df.select([(f.count(f.when(f.col(c).isNull(), c))/count_before).alias(c) for c in df.columns])\n",
        "scheme = df.columns\n",
        "null_distr = over_thresh_col.collect()[0].asDict().values()\n",
        "for i in np.where(np.array(list(null_distr)) > ((1 - drop_thresh)))[0]:\n",
        "  df = df.drop(scheme[i])\n",
        "print('Dropped {} columns with less than {}% of non-null values'.format(col_before-len(df.columns), drop_thresh*100))\n",
        "# print(f\"The shape is {df.count():d} rows by {len(df.columns):d} columns.\")\n",
        "print('==> Done\\n')"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dropping useless columns, columns and rows with more than 66.0% of null values\n",
            "Dropped 0 rows with less than 66.0% of non-null values\n",
            "Keeping only rows with non-null value in target position\n",
            "Dropped 11893 rows with with null in WAGP column\n",
            "Dropped 47 columns with less than 66.0% of non-null values\n",
            "==> Done\n",
            "\n",
            "CPU times: user 1.25 s, sys: 325 ms, total: 1.58 s\n",
            "Wall time: 2min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnqUpncPSWFq"
      },
      "source": [
        "df.select(label_to_predict).describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fREFKGcDI-vo"
      },
      "source": [
        "%%time\n",
        "print('Removing outliers')\n",
        "df = remove_outliers(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSAFdAyu2nzE"
      },
      "source": [
        "%%time\n",
        "imput_strategy = 'mean'\n",
        "print('Filling remaining null values with {} of each column'.format(imput_strategy))\n",
        "imputer = Imputer() # Filling missing values with mean of the column\n",
        "imputer.setInputCols(df.columns)\n",
        "imputer.setOutputCols(df.columns)\n",
        "imputer.setStrategy(imput_strategy)\n",
        "df = imputer.fit(df).transform(df)\n",
        "print('==> Done\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Q3lUZDTR7Z"
      },
      "source": [
        "df.select(label_to_predict).describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkK8YRKBlNWT",
        "outputId": "7efcc31c-cd43-47a8-a693-f3fcad3cc2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%time\n",
        "print('Rounding dataset')\n",
        "progressbar = ProgressBar()\n",
        "for col_name in progressbar(df.columns):\n",
        "    df = df.withColumn(col_name, f.round(f.col(col_name), 3))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0% (2 of 234) |                        | Elapsed Time: 0:00:00 ETA:   0:00:12"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Rounding dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (234 of 234) |######################| Elapsed Time: 0:00:25 Time:  0:00:25\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 670 ms, sys: 146 ms, total: 816 ms\n",
            "Wall time: 25.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX-LX2Cy2nzO"
      },
      "source": [
        "%%time\n",
        "print('Choosing top features, assembling and scaling values')\n",
        "feature_columns = df.columns\n",
        "feature_columns.remove(label_to_predict)\n",
        "features_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
        "feature_selector = ChiSqSelector(numTopFeatures=50, featuresCol=\"features\", outputCol=\"selected_features\", labelCol=label_to_predict)\n",
        "target_assembler = VectorAssembler(inputCols=[label_to_predict], outputCol='target')\n",
        "features_standardscaler = StandardScaler(inputCol='selected_features', outputCol='scaled_features', withStd=True, withMean=True)\n",
        "df = Pipeline(stages=[features_assembler, target_assembler, feature_selector, features_standardscaler]).fit(df).transform(df)\n",
        "print(f\"The shape is {df.count():d} rows by {feature_selector.getNumTopFeatures():d} columns.\")\n",
        "print('==> Done\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zsADoPQ2nzZ"
      },
      "source": [
        "%%time\n",
        "print('Creating labeled points')\n",
        "to_label_df = df.select('target', 'scaled_features')\n",
        "labeled_data = to_label_df.rdd.map(lambda row: to_labeledpoint(row))\n",
        "print('Splitting in training set, validation set and test set')\n",
        "labeled_train_df, labeled_validation_df, labeled_test_df = labeled_data.randomSplit(weights=[.6, .2, .2], seed=6)\n",
        "labeled_train_df = labeled_train_df.repartition(num_partitions)\n",
        "labeled_validation_df = labeled_validation_df.repartition(num_partitions)\n",
        "labeled_test_df = labeled_test_df.repartition(num_partitions)\n",
        "labeled_train_df.cache()\n",
        "labeled_validation_df.cache()\n",
        "labeled_test_df.cache()\n",
        "print('Training set count: {} divided in {} partitions'.format(labeled_train_df.count(), labeled_train_df.getNumPartitions()))\n",
        "print('Validation set count: {} divided in {} partitions'.format(labeled_validation_df.count(), labeled_validation_df.getNumPartitions()))\n",
        "print('Test set count: {} divided in {} partitions'.format(labeled_test_df.count(), labeled_test_df.getNumPartitions()))\n",
        "print('==> Done\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWo58RTF6YpR"
      },
      "source": [
        "print('Training and predictions will work on ')\n",
        "labeled_data.take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JnyRXvNB8ID"
      },
      "source": [
        "# Best model individuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZQN11Tb2-xp"
      },
      "source": [
        "%%time\n",
        "from progressbar import ProgressBar\n",
        "progressbar = ProgressBar()\n",
        "print('Finding regularization factor using grid search')\n",
        "best_reg_factor = 0\n",
        "mean_squared_errors = []\n",
        "for candidate_reg_factor in progressbar(alphas):\n",
        "  print('Trying using {} as regularization factor'.format(candidate_reg_factor))\n",
        "  mean_squared_error = 0\n",
        "  candidate_rr = SparkRidgeRegression(n_iterations=n_iterations, learning_rate=learning_rate, reg_factor=candidate_reg_factor)\n",
        "  candidate_rr.fit(labeled_train_df)\n",
        "  print('Validating')\n",
        "  candidate_pred = labeled_validation_df.map(lambda prediction: candidate_rr.predict(prediction))\n",
        "  mean_squared_error = candidate_rr.mean_squared_error(predictions=candidate_pred)\n",
        "  mean_squared_errors.append(mean_squared_error)\n",
        "  if mean_squared_error < lowest_error:\n",
        "    print('Currently best regularization factor: {}'.format(candidate_reg_factor))\n",
        "    best_reg_factor = candidate_reg_factor\n",
        "    lowest_error = mean_squared_error\n",
        "print('Grid search terminated, chosen regularization factor: {}'.format(best_reg_factor))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz1adksczoXG",
        "outputId": "66781a55-aefd-4521-fb90-a80358f26e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "print('MSE with different regularization factors')\n",
        "sn.lineplot(alphas, mean_squared_errors).grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MSE with different regularization factors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9iuu2vCCBx3"
      },
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oveBRH_2nzs"
      },
      "source": [
        "%%time\n",
        "print('Training with {} as learning rate and {} as regularization factor'.format(learning_rate, 0.1))\n",
        "spark_rr = SparkRidgeRegression(n_iterations=300, learning_rate=0.00001, reg_factor=0.01)\n",
        "now = datetime.now()\n",
        "spark_rr.fit(labeled_train_df)\n",
        "print('==> Done in {}\\n'.format(datetime.now()-now))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koNqmnzQSNgW"
      },
      "source": [
        "%%time\n",
        "print('Predicting')\n",
        "predictions = labeled_test_df.map(lambda prediction: spark_rr.predict(prediction))\n",
        "float_predictions = predictions.map(lambda xs: [float(x) for x in xs])\n",
        "predictions_df = float_predictions.toDF(['label', 'predictions'])\n",
        "print('==> Done\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFqhWTUh2n0B"
      },
      "source": [
        "%%time\n",
        "root_mean_squared_error = spark_rr.root_mean_squared_error(predictions=predictions)\n",
        "mean_squared_error = spark_rr.mean_squared_error(predictions=predictions)\n",
        "mean_absolute_error = spark_rr.mean_absolute_error(predictions=predictions)\n",
        "r2_score = spark_rr.r2(predictions_df)\n",
        "print('From scratch test MSE: {}'.format(mean_squared_error))\n",
        "print('From scratch test RMSE: {}'.format(root_mean_squared_error))\n",
        "print('From scratch test MAE: {}'.format(mean_absolute_error))\n",
        "print('From scratch test R2: {}'.format(r2_score))\n",
        "print('==> Done\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seu2MrQK2n0L"
      },
      "source": [
        "fig = plt.figure(figsize = (12, 7))\n",
        "g = sn.lineplot(x = np.arange(spark_rr.n_iterations), y = spark_rr.training_errors, color = 'crimson').grid()\n",
        "fig.suptitle('Plotting training error during GD iterations', fontsize = 20)\n",
        "plt.xlabel('Iteration', fontsize = 14)\n",
        "plt.ylabel('Training error', fontsize = 14)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}